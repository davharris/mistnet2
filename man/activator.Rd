% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activator.R
\docType{data}
\name{elu_activator}
\alias{elu_activator}
\alias{identity_activator}
\alias{relu_activator}
\alias{sigmoid_activator}
\title{Activator objects and activation functions for nonlinearities and their gradients
in neural networks}
\format{An object of class \code{activator} of length 3, containing
functions relating to neural network nonlinearities:
\itemize{
   \item{\code{f}: The activation function (nonlinearity) itself}
   \item{\code{grad}: The activation function's gradient with respect to
       \code{x}}
   \item{\code{initialize_activatorinal_biases}: A function used internally to
       initialize the biases of a network's output layer}
}}
\usage{
elu_activator

identity_activator

relu_activator

sigmoid_activator
}
\description{
Activator objects and activation functions for nonlinearities and their gradients
in neural networks
}
\details{
The following activators/activation functions are currently included:
\itemize{
   \item{elu}: "exponential linear unit" \code{f(x)=x} when x>0 and \code{f(x)=exp(x)-1} otherwise
   \item{identity}: the identity function
   \item{relu}: "rectified linear unit" \code{f(x)=x} when x>0 and \code{f(x)=0} otherwise
   \item{sigmoid}: sigmoid (logistic) function, 1/(1 + exp(-x))
}
}
\keyword{datasets}

