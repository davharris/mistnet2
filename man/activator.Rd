% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activator.R
\docType{data}
\name{elu_activator}
\alias{elu_activator}
\alias{identity_activator}
\alias{relu_activator}
\alias{sigmoid_activator}
\title{Activator objects and activation functions for nonlinearities and their gradients
in neural networks}
\format{An object of class \code{activator} of length 3, containing
functions relating to neural network nonlinearities:
\itemize{
   \item{\code{f}: The activation function (nonlinearity) itself}
   \item{\code{grad}: The activation function's gradient with respect to
       \code{x}}
   \item{\code{initialize_activatorinal_biases}: A function used internally to
       initialize the biases of a network's output layer}
}}
\usage{
elu_activator

identity_activator

relu_activator

sigmoid_activator
}
\description{
Activator objects and activation functions for nonlinearities and their gradients
in neural networks
}
\details{
The following activators/activation functions are currently included:
\itemize{
   \item{elu}: "exponential linear unit" \code{f(x)=x} when x>0 and \code{f(x)=exp(x)-1} otherwise. See Clevert et al. (2015)
   \item{identity}: the identity function, \code{f(x)=x}
   \item{relu}: "rectified linear unit" \code{f(x)=x} when x>0 and \code{f(x)=0} otherwise. See Nair and Hinton (2010).
   \item{sigmoid}: sigmoid (logistic) function, \code{f(x)=1/(1 + exp(-x))}
}
}
\references{
Clevert, Djork-Arn√©, Thomas Unterthiner, and Sepp Hochreiter. "Fast and
Accurate Deep Network Learning by Exponential Linear Units (ELUs)."
arXiv preprint arXiv:1511.07289 (2015). Harvard

Nair, Vinod, and Geoffrey E. Hinton. "Rectified linear units improve
restricted boltzmann machines." In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pp. 807-814. 2010.
}
\keyword{datasets}

