% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activator.R
\docType{data}
\name{elu_activator}
\alias{activator}
\alias{elu_activator}
\alias{exp_activator}
\alias{identity_activator}
\alias{relu_activator}
\alias{sigmoid_activator}
\title{Activator objects and nonlinear activation functions}
\format{An object of class \code{activator} of length 4, containing the name of the
activation function (a character vector) and three functions relating
to neural network nonlinearities:
\itemize{
   \item{\code{f}: The activation function (nonlinearity) itself}
   \item{\code{grad}: The activation function's gradient with respect to
       \code{x}}
   \item{\code{initialize_activator_biases}: A function used internally to
       initialize the biases of a network's output layer}
}}
\usage{
elu_activator

identity_activator

relu_activator

sigmoid_activator

exp_activator
}
\description{
Activator objects and nonlinear activation functions
}
\details{
The following activators/activation functions are currently included:

\itemize{
   \item{elu}: "exponential linear unit" \code{f(x)=x} when x>0 and \code{f(x)=exp(x)-1} otherwise. See Clevert et al. (2015)
   \item{identity}: the identity function, \code{f(x)=x}
   \item{relu}: "rectified linear unit" \code{f(x)=x} when x>0 and \code{f(x)=0} otherwise. See Nair and Hinton (2010).
   \item{sigmoid}: sigmoid (logistic) function, \code{f(x)=1/(1 + exp(-x))}
}

The sigmoid activation function used to be the most common in neural networks.
Most modern networks use relus, although Clevert et al. (2015) have recently
shown that elus can work better because it can produce negative values.
}
\note{
The C++ code used to speed up some activation functions (relu and elu)
   assumes that they are passed \code{matrix} objects, rather than vectors.
}
\examples{
x = matrix(seq(-4, 4, .01), ncol = 1)
par(mfrow = c(2, 2))
plot(x = x, y = elu_activator$f(x), type = "l", main = "elu")
plot(x = x, y = identity_activator$f(x), type = "l", main = "identity")
plot(x = x, y = relu_activator$f(x), type = "l", main = "relu")
plot(x = x, y = sigmoid_activator$f(x), type = "l", main = "sigmoid")
abline(h = 0, col = "gray")
}
\references{
Clevert, Djork-Arn√©, Thomas Unterthiner, and Sepp Hochreiter. "Fast and
Accurate Deep Network Learning by Exponential Linear Units (ELUs)."
arXiv preprint arXiv:1511.07289 (2015). Harvard

Nair, Vinod, and Geoffrey E. Hinton. "Rectified linear units improve
restricted boltzmann machines." In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pp. 807-814. 2010.
}
\keyword{datasets}

